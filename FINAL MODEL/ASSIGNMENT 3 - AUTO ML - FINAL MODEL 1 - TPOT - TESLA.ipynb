{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n",
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20000000298023224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10000000149011612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10000000149011612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          google score\n",
       "0                  0.0\n",
       "1  0.20000000298023224\n",
       "2  0.10000000149011612\n",
       "3                  0.0\n",
       "4  0.10000000149011612"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google = []\n",
    "with open('google_tesla.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        google.append(rows)\n",
    "\n",
    "google = google[1:]\n",
    "google = pd.DataFrame(google,columns=['id','google score'])\n",
    "\n",
    "google_ml = google[['google score']]\n",
    "\n",
    "google_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aws_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.43361134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.76289069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.80252462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.63196903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90996217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    aws_score\n",
       "0  3.43361134\n",
       "1  0.76289069\n",
       "2  0.80252462\n",
       "3  0.63196903\n",
       "4  0.90996217"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws = []\n",
    "with open('aws_tesla.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        aws.append(rows)\n",
    "\n",
    "aws = aws[1:]\n",
    "aws = pd.DataFrame(aws,columns=['id','aws_score',])\n",
    "\n",
    "aws_ml = aws[['aws_score']]\n",
    "\n",
    "aws_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>watson_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  watson_score\n",
       "0          0.0\n",
       "1         0.74\n",
       "2          0.0\n",
       "3         0.75\n",
       "4         -0.7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watson = []\n",
    "with open('watson_tesla.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        watson.append(rows)\n",
    "\n",
    "watson = watson[1:]\n",
    "watson = pd.DataFrame(watson,columns=['id','watson_score'])\n",
    "\n",
    "watson_ml = watson[['watson_score']]\n",
    "\n",
    "watson_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>azure score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  azure score\n",
       "0       0.5  \n",
       "1       0.837\n",
       "2       0.5  \n",
       "3       0.958\n",
       "4       0.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure = []\n",
    "with open('azure_tesla.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        azure.append(rows)\n",
    "\n",
    "azure = azure[1:]\n",
    "azure = pd.DataFrame(azure,columns=['id','azure score'])\n",
    "\n",
    "azure_ml = azure[['azure score']]\n",
    "\n",
    "azure_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manual_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manual_sentiment\n",
       "0                0\n",
       "1                1\n",
       "2                1\n",
       "3                1\n",
       "4                0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual = []\n",
    "with open('manual_tesla.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        manual.append(rows)\n",
    "\n",
    "manual = manual[1:]\n",
    "manual = pd.DataFrame(manual,columns=['id','manual_sentiment'])\n",
    "\n",
    "manual_ml = manual[['manual_sentiment']]\n",
    "#manual_ml = manual_ml.replace('0','0.5')\n",
    "#manual_ml = manual_ml.replace('-1','0')\n",
    "manual_ml.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = google_ml.join(aws_ml)\n",
    "d = d.join(azure_ml)\n",
    "d = d.join(watson_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "lbl = LabelEncoder()\n",
    "q = lbl.fit_transform(manual_ml)\n",
    "d = pd.get_dummies(d).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(d,q,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
      "29 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb89417b2d84f6b9e8a8725d0fc8a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.6141379310344828\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.6141379310344828\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "-2\t0.6220197044334975\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #324 due to time out. Continuing to the next pipeline.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.6212972085385878\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.6220197044334975\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.6212972085385878\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.6291461412151067\tLinearSVC(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=14, GradientBoostingClassifier__min_samples_split=18, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001), LinearSVC__C=0.1, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=1e-05)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.6212972085385878\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.6429556650246304\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90\n",
      "Skipped pipeline #660 due to time out. Continuing to the next pipeline.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.6220032840722495\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.6429556650246304\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.6220032840722495\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\t0.6429556650246304\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=15)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-3\t0.6498522167487684\tDecisionTreeClassifier(ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.35000000000000003, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=16, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=11, DecisionTreeClassifier__min_samples_split=19)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Generation 11 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-3\t0.6498522167487684\tDecisionTreeClassifier(ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.35000000000000003, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=16, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=11, DecisionTreeClassifier__min_samples_split=19)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Generation 12 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-2\t0.6496223316912972\tRandomForestClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.15000000000000002, ExtraTreesClassifier__min_samples_leaf=12, ExtraTreesClassifier__min_samples_split=15, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.6500985221674875\tDecisionTreeClassifier(Normalizer(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.15000000000000002, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), Normalizer__norm=l1), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=14)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 13 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-2\t0.6496223316912972\tRandomForestClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.15000000000000002, ExtraTreesClassifier__min_samples_leaf=12, ExtraTreesClassifier__min_samples_split=15, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.6500985221674875\tDecisionTreeClassifier(Normalizer(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.15000000000000002, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), Normalizer__norm=l1), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=14)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 14 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-2\t0.6498522167487684\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=15, ExtraTreesClassifier__min_samples_split=12, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=14)\n",
      "-3\t0.6500985221674875\tDecisionTreeClassifier(Normalizer(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.15000000000000002, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), Normalizer__norm=l1), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=14)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-2\t0.6498522167487684\tDecisionTreeClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=15, ExtraTreesClassifier__min_samples_split=12, ExtraTreesClassifier__n_estimators=100), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=14)\n",
      "-3\t0.6500985221674875\tDecisionTreeClassifier(Normalizer(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.15000000000000002, ExtraTreesClassifier__min_samples_leaf=14, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), Normalizer__norm=l1), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=14)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 16 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-2\t0.6500985221674875\tGradientBoostingClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 17 - Current Pareto front scores:\n",
      "-1\t0.6434482758620689\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.6500000000000001)\n",
      "-2\t0.6500985221674875\tGradientBoostingClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 18 - Current Pareto front scores:\n",
      "-1\t0.6500985221674875\tGradientBoostingClassifier(CombineDFs(input_matrix, input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Generation 19 - Current Pareto front scores:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\t0.6500985221674875\tGradientBoostingClassifier(CombineDFs(input_matrix, input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Generation 20 - Current Pareto front scores:\n",
      "-1\t0.6500985221674875\tGradientBoostingClassifier(CombineDFs(input_matrix, input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Generation 21 - Current Pareto front scores:\n",
      "-1\t0.6500985221674875\tGradientBoostingClassifier(CombineDFs(input_matrix, input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=100,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=100,\n",
       "        random_state=None, scoring='accuracy', subsample=1.0,\n",
       "        use_dask=False, verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(verbosity=3, \n",
    "                      scoring=\"accuracy\"\n",
    "                      )\n",
    "\n",
    "tpot.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3888888888888889"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEICAYAAAD8yyfzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb3f6dc128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict = tpot.predict(X_test)\n",
    "cm = confusion_matrix(y_test,predict)\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel('Predicted Sentiments')\n",
    "ax.set_ylabel('Manual Sentiments')\n",
    "ax.set_title('Confusion Matrix')\n",
    "sn.heatmap(cm, annot=True,annot_kws={\"size\": 16})# font size\n",
    "ax.xaxis.set_ticklabels(['Negative','Neutral','Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative','Neutral','Positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST PREDICTION ANALYSIS :\n",
    "#### 1 out of 4 Negative labels were predicted correctly ; 3 out of 11 Neutral labels are predicted correctly ;10 out of 21 Positive labels are predicted correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAEICAYAAAAeFzyKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb3f5f4160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_train = tpot.predict(X_train)\n",
    "cm_train = confusion_matrix(y_train,predict_train)\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel('Predicted Sentiments')\n",
    "ax.set_ylabel('Manual Sentiments')\n",
    "ax.set_title('Confusion Matrix')\n",
    "sn.heatmap(cm_train, annot=True,annot_kws={\"size\": 16})# font size\n",
    "ax.xaxis.set_ticklabels(['Negative','Neutral','Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative','Neutral','Positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN PREDICTION ANALYSIS :\n",
    "#### 3 out of 17 Negative labels were predicted correctly ; 32 out of 55 Neutral labels are predicted correctly ; 60 out of 71 Positive labels are predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT FINAL PREDICTION ANALYSIS :\n",
    "#### Negative  - 4 out of 21 ; Neutral - 35 out of 66 ; Positive -70 out of 81 correctly predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
